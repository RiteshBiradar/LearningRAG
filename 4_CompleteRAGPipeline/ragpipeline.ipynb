{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8cb0e9",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f47859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = [\n",
    "  \"\"\"Deep learning is a subfield of machine learning that uses neural networks with multiple layers. It is widely applied in image recognition, natural language processing, and speech recognition.\"\"\",\n",
    "\n",
    "  \"\"\"Machine learning is the process of teaching computers to make decisions without being explicitly programmed. It includes supervised, unsupervised, and reinforcement learning approaches.\"\"\",\n",
    "\n",
    "  \"\"\"Retrieval-Augmented Generation (RAG) combines a retrieval system with a generative model. It improves accuracy by retrieving relevant documents from a knowledge base before generating answers.\"\"\",\n",
    "\n",
    "  \"\"\"LangChain is a framework designed to build applications powered by large language models. It provides abstractions for chains, agents, and memory, making it easier to integrate LLMs with external data.\"\"\",\n",
    "\n",
    "  \"\"\"When using RAG with LangChain, the pipeline typically involves embedding documents, storing them in a vector database, retrieving relevant chunks, and passing them into the LLM for context-aware generation.\"\"\",\n",
    "\n",
    "  \"\"\"Popular vector databases used with LangChain for RAG pipelines include Pinecone, Weaviate, FAISS, and Milvus.\"\"\",\n",
    "\n",
    "  \"\"\"Deep learning models often require GPUs for efficient training because of their high computational requirements.\"\"\",\n",
    "\n",
    "  \"\"\"LangChain supports integration with multiple LLM providers, including OpenAI, Anthropic, and Hugging Face.\"\"\",\n",
    "\n",
    "  \"\"\"In a RAG pipeline, embeddings are numerical vector representations of text that allow semantic similarity search.\"\"\",\n",
    "\n",
    "  \"\"\"Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets to improve accuracy.\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff9ed1",
   "metadata": {},
   "source": [
    "### Document Loading\n",
    "- But as I am using a dummy data over here, we can skip this part , or store it in a folder data and use directoryloader to import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976dd5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Deep learning is a subfield of machine learning that uses neural networks with multiple layers. It is widely applied in image recognition, natural language processing, and speech recognition.'),\n",
       " Document(metadata={}, page_content='Machine learning is the process of teaching computers to make decisions without being explicitly programmed. It includes supervised, unsupervised, and reinforcement learning approaches.'),\n",
       " Document(metadata={}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a generative model. It improves accuracy by retrieving relevant documents from a knowledge base before generating answers.'),\n",
       " Document(metadata={}, page_content='LangChain is a framework designed to build applications powered by large language models. It provides abstractions for chains, agents, and memory, making it easier to integrate LLMs with external data.'),\n",
       " Document(metadata={}, page_content='When using RAG with LangChain, the pipeline typically involves embedding documents, storing them in a vector database, retrieving relevant chunks, and passing them into the LLM for context-aware generation.'),\n",
       " Document(metadata={}, page_content='Popular vector databases used with LangChain for RAG pipelines include Pinecone, Weaviate, FAISS, and Milvus.'),\n",
       " Document(metadata={}, page_content='Deep learning models often require GPUs for efficient training because of their high computational requirements.'),\n",
       " Document(metadata={}, page_content='LangChain supports integration with multiple LLM providers, including OpenAI, Anthropic, and Hugging Face.'),\n",
       " Document(metadata={}, page_content='In a RAG pipeline, embeddings are numerical vector representations of text that allow semantic similarity search.'),\n",
       " Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets to improve accuracy.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "documents = [Document(page_content=doc) for doc in sample_docs] \n",
    "documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16934af",
   "metadata": {},
   "source": [
    "### Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e8f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 23\n",
      "page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a generative model. It'\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(chunks[4])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccceddd",
   "metadata": {},
   "source": [
    "### Embeddings using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c116ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rites\\Desktop\\RAG\\RAG\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03447727486491203, 0.03102317824959755, 0.006734970025718212, 0.026108985766768456, -0.03936202451586723, -0.16030244529247284, 0.06692401319742203, -0.006441489793360233, -0.0474504791200161, 0.014758856035768986, 0.07087527960538864, 0.05552763119339943, 0.019193334504961967, -0.026251312345266342, -0.01010954286903143, -0.02694045566022396, 0.022307461127638817, -0.022226648405194283, -0.14969263970851898, -0.017493007704615593, 0.00767625542357564, 0.05435224249958992, 0.0032543970737606287, 0.031725890934467316, -0.0846213847398758, -0.02940601296722889, 0.05159561336040497, 0.04812406003475189, -0.0033148222137242556, -0.058279167860746384, 0.04196927323937416, 0.022210685536265373, 0.1281888335943222, -0.022338971495628357, -0.011656315997242928, 0.06292839348316193, -0.032876335084438324, -0.09122604131698608, -0.031175347045063972, 0.0526994913816452, 0.04703482985496521, -0.08420311659574509, -0.030056199058890343, -0.02074483036994934, 0.009517835453152657, -0.0037217906210571527, 0.007343285251408815, 0.03932438790798187, 0.0932740643620491, -0.003788596484810114, -0.052742067724466324, -0.05805816128849983, -0.006864361464977264, 0.005283191800117493, 0.0828929990530014, 0.019362755119800568, 0.0062844837084412575, -0.010330787859857082, 0.009032378904521465, -0.037683695554733276, -0.04520607739686966, 0.024016305804252625, -0.006944137159734964, 0.013491630554199219, 0.10005494207143784, -0.07168391346931458, -0.021695120260119438, 0.031618405133485794, -0.051634665578603745, -0.08224772661924362, -0.06569333374500275, -0.00989533495157957, 0.005816374905407429, 0.07355456054210663, -0.034050311893224716, 0.0248861201107502, 0.014488042332231998, 0.02645738422870636, 0.009656722657382488, 0.0302172489464283, 0.05280393362045288, -0.07535984367132187, 0.009897145442664623, 0.029836809262633324, 0.01755557768046856, 0.023091984912753105, 0.001933806692250073, 0.0014002545503899455, -0.04717595875263214, -0.011194315738976002, -0.11420144140720367, -0.019811924546957016, 0.040266189724206924, 0.0021929906215518713, -0.07979220896959305, -0.02538231760263443, 0.09448299556970596, -0.02898104302585125, -0.14500252902507782, 0.23097744584083557, 0.027731187641620636, 0.03211146965622902, 0.03106505796313286, 0.04283284768462181, 0.06423777341842651, 0.03216316178441048, -0.004876770544797182, 0.055699463933706284, -0.03753238171339035, -0.02150554023683071, -0.028342634439468384, -0.028846951201558113, 0.0383530892431736, -0.017468664795160294, 0.052485305815935135, -0.07487601786851883, -0.03125976398587227, 0.021841565147042274, -0.03989570587873459, -0.008587091229856014, 0.026956576853990555, -0.04849553853273392, 0.011469882912933826, 0.02961820363998413, -0.02057218924164772, 0.013103843666613102, 0.028833510354161263, -3.1941990848222185e-33, 0.06478213518857956, -0.018130183219909668, 0.051789961755275726, 0.12198275327682495, 0.028780106455087662, 0.008721951395273209, -0.07052119821310043, -0.016907278448343277, 0.04073973000049591, 0.042116157710552216, 0.025447236374020576, 0.03574628755450249, -0.04914471507072449, 0.0021290204022079706, -0.015546582639217377, 0.050730545073747635, -0.0481853261590004, 0.03588061034679413, -0.0040670474991202354, 0.10172472149133682, -0.05597002059221268, -0.010681048966944218, 0.01123578567057848, 0.09068653732538223, 0.004234451334923506, 0.035138655453920364, -0.009702847339212894, -0.09386517852544785, 0.0928555428981781, 0.008004927076399326, -0.007705425377935171, -0.05208674445748329, -0.012587991543114185, 0.0032669377978891134, 0.006013509817421436, 0.007581559009850025, 0.01051718182861805, -0.08634556829929352, -0.06987880170345306, -0.0025338928680866957, -0.09097658842802048, 0.04688733071088791, 0.052076540887355804, 0.007193844299763441, 0.010903622955083847, -0.0052295587956905365, 0.013937311246991158, 0.021968349814414978, 0.03420866280794144, 0.060224682092666626, 0.00011665470083244145, 0.014731976203620434, -0.07008926570415497, 0.028499048203229904, -0.02760172076523304, 0.010768445208668709, 0.034830961376428604, -0.02248787134885788, 0.009769017808139324, 0.07722785323858261, 0.021588314324617386, 0.11495620757341385, -0.0680011734366417, 0.02376098558306694, -0.0159839428961277, -0.0178269911557436, 0.06439495831727982, 0.032025739550590515, 0.05027025192975998, -0.005913770757615566, -0.03370805084705353, 0.017840256914496422, 0.016573317348957062, 0.06329657882452011, 0.03467721864581108, 0.046473488211631775, 0.09790610522031784, -0.00663550291210413, 0.02520712837576866, -0.07798824459314346, 0.0169264767318964, -0.000945797364693135, 0.022471921518445015, -0.038253191858530045, 0.09570474177598953, -0.005350803025066853, 0.010469110682606697, -0.11524055153131485, -0.013262521475553513, -0.010709455236792564, -0.08311725407838821, 0.07327353954315186, 0.04939225688576698, -0.008994322270154953, -0.09584552049636841, 3.3661485617505796e-33, 0.12493184208869934, 0.01934972032904625, -0.05822571739554405, -0.03598826378583908, -0.05074676498770714, -0.04566238448023796, -0.08260336518287659, 0.1481948047876358, -0.08842118829488754, 0.06027443706989288, 0.05103015899658203, 0.01030314713716507, 0.14121422171592712, 0.03081384487450123, 0.061033159494400024, -0.052851270884275436, 0.1366489678621292, 0.00918989721685648, -0.01732518896460533, -0.012848555110394955, -0.007995282299816608, -0.0509800985455513, -0.05235064774751663, 0.007593012880533934, -0.015166307799518108, 0.01696030981838703, 0.021270520985126495, 0.020558107644319534, -0.12002813816070557, 0.014461833983659744, 0.02675991877913475, 0.025330696254968643, -0.0427546352148056, 0.006768387276679277, -0.01445858459919691, 0.04526195675134659, -0.09147648513317108, -0.019439145922660828, -0.017833467572927475, -0.05491018295288086, -0.052641112357378006, -0.010459048673510551, -0.052016086876392365, 0.020891955122351646, -0.07997036725282669, -0.012111340649425983, -0.05773142725229263, 0.023178234696388245, -0.008031732402741909, -0.02598930336534977, -0.07995671033859253, -0.020728832110762596, 0.048817697912454605, -0.020389137789607048, -0.04917657747864723, 0.014159622602164745, -0.06362202018499374, -0.007807393092662096, 0.01643155701458454, -0.0256824791431427, 0.013381040655076504, 0.026248741894960403, 0.009978413581848145, 0.06322886794805527, 0.002672201255336404, -0.006582767236977816, 0.01663188263773918, 0.03236646577715874, 0.03794245794415474, -0.036376070231199265, -0.006910930387675762, 0.0001596928050275892, -0.0016335808904841542, -0.02727821283042431, -0.02803807333111763, 0.049681417644023895, -0.028867173939943314, -0.0024180689360946417, 0.014774908311665058, 0.009764534421265125, 0.005797638092190027, 0.013486160896718502, 0.0055678957141935825, 0.03722710534930229, 0.007232527248561382, 0.04015626385807991, 0.08150326460599899, 0.07199167460203171, -0.013056126423180103, -0.0428820475935936, -0.01101123820990324, 0.004897820297628641, -0.009229730814695358, 0.035191506147384644, -0.05103502422571182, -1.571437557856825e-08, -0.08862441033124924, 0.02390925958752632, -0.01623876392841339, 0.031700510531663895, 0.027284247800707817, 0.05246882885694504, -0.047070957720279694, -0.058847445994615555, -0.06320822983980179, 0.04088849574327469, 0.04982800409197807, 0.10655171424150467, -0.07450230419635773, -0.012495421804487705, 0.01837071217596531, 0.03947412595152855, -0.024797886610031128, 0.014516262337565422, -0.03706921637058258, 0.02001572772860527, -4.85817035951186e-05, 0.00986657664179802, 0.024838753044605255, -0.05245814099907875, 0.029314178973436356, -0.08719190955162048, -0.01449968758970499, 0.026019077748060226, -0.01874636672437191, -0.07620512694120407, 0.03504333272576332, 0.10363949835300446, -0.028050510212779045, 0.012718182988464832, -0.07632549107074738, -0.01865232177078724, 0.02497672848403454, 0.0814453512430191, 0.06875883787870407, -0.0640566498041153, -0.08389385789632797, 0.06136231869459152, -0.033545564860105515, -0.10615336894989014, -0.040080588310956955, 0.032530225813388824, 0.07662483304738998, -0.0730162113904953, 0.00033755265758372843, -0.040871646255254745, -0.0757884755730629, 0.027527665719389915, 0.07462543249130249, 0.01771726831793785, 0.09121846407651901, 0.11022016406059265, 0.0005697731394320726, 0.05146336182951927, -0.014551310800015926, 0.03323203697800636, 0.023792240768671036, -0.02288980595767498, 0.038937538862228394, 0.030206844210624695]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") \n",
    "vector = embeddings.embed_query(\"Hello world\")\n",
    "print(vector)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880b740",
   "metadata": {},
   "source": [
    "### Initalize ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78443eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the collection: 69\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "persist_dir = \"./chroma_db\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"rag_collection\",\n",
    "    persist_directory=persist_dir,\n",
    ")\n",
    "vector_db.persist()\n",
    "\n",
    "print(f\"Number of documents in the collection: {vector_db._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107252d",
   "metadata": {},
   "source": [
    "### Testing Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea35b2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets'),\n",
       " Document(metadata={}, page_content='datasets to improve accuracy.'),\n",
       " Document(metadata={}, page_content='programmed. It includes supervised, unsupervised, and reinforcement learning approaches.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Fine tuning?\"\n",
    "result = vector_db.similarity_search(query,k=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8a83e",
   "metadata": {},
   "source": [
    "### Advance Similarity Search with Scores\n",
    "- Lower the value closer to the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466210b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets'),\n",
       "  1.1072428226470947),\n",
       " (Document(metadata={}, page_content='datasets to improve accuracy.'),\n",
       "  1.422195315361023),\n",
       " (Document(metadata={}, page_content='programmed. It includes supervised, unsupervised, and reinforcement learning approaches.'),\n",
       "  1.5049494504928589)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = vector_db.similarity_search_with_score(query,k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf2ae7",
   "metadata": {},
   "source": [
    "### Initalizing LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31b82323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001BE10A27A00>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc37a6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An **LLM** stands for **Large Language Model**.\\n\\nIt is a type of artificial intelligence (AI) model that has been trained on a massive amount of text data to understand, generate, and process human language.\\n\\nLet\\'s break down each part of the name:\\n\\n1.  **Large:** This refers to two main things:\\n    *   **Parameters:** LLMs have billions, sometimes even trillions, of parameters (the internal variables the model learns during training). More parameters generally allow the model to capture more complex patterns and nuances in language.\\n    *   **Training Data:** They are trained on incredibly vast datasets, often comprising a significant portion of the internet (web pages, books, articles, code, etc.).\\n\\n2.  **Language:** This indicates their primary domain. LLMs are designed to work with human language in various forms:\\n    *   **Understanding:** Interpreting the meaning, context, and intent of text.\\n    *   **Generating:** Producing coherent, relevant, and human-like text.\\n    *   **Processing:** Analyzing, summarizing, translating, and transforming text.\\n\\n3.  **Model:** This refers to the computational structure and algorithms that learn from the data. LLMs are typically built using deep neural networks, most famously the **Transformer architecture**, which allows them to efficiently process long sequences of text and understand relationships between words regardless of their distance in a sentence.\\n\\n**How They Work (Simplified):**\\n\\nAt its core, an LLM\\'s fundamental task is to predict the next word in a sequence given the preceding words. By repeatedly performing this prediction over massive amounts of text, the model learns:\\n*   Grammar and syntax\\n*   Facts and common knowledge\\n*   Reasoning patterns\\n*   Different writing styles and tones\\n*   The relationships between concepts\\n\\n**Key Characteristics & Capabilities:**\\n\\n*   **Generative:** They can create original content, from essays and emails to code and creative stories.\\n*   **Contextual Understanding:** They can understand the meaning of words and phrases based on the surrounding text.\\n*   **Versatility:** With a single model, they can perform a wide range of tasks (e.g., summarization, translation, question answering, brainstorming) without needing to be specifically retrained for each task, often just by being given clear instructions (a \"prompt\").\\n*   **Few-shot/Zero-shot Learning:** They can perform new tasks effectively with very few (few-shot) or even no (zero-shot) examples, relying on their general understanding of language.\\n\\n**Common Applications:**\\n\\n*   **Chatbots and Conversational AI:** Powering intelligent assistants and customer service bots.\\n*   **Content Creation:** Generating articles, marketing copy, social media posts, and creative writing.\\n*   **Summarization:** Condensing long documents into shorter, key points.\\n*   **Translation:** Translating text between different languages.\\n*   **Question Answering:** Providing direct answers to questions.\\n*   **Coding Assistance:** Generating code, debugging, and explaining programming concepts.\\n*   **Data Analysis:** Extracting information, categorizing text, and identifying patterns.\\n\\n**Examples of LLMs:**\\n\\n*   **GPT series (OpenAI):** GPT-3, GPT-3.5, GPT-4 (powers ChatGPT)\\n*   **Gemini (Google)**\\n*   **Llama series (Meta)**\\n*   **Claude (Anthropic)**\\n\\nIn essence, LLMs are powerful AI systems that have revolutionized how we interact with and create text, enabling a vast array of new applications and possibilities.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--982b0d1e-0d05-414c-a49c-3ecc99d3f82e-0', usage_metadata={'input_tokens': 6, 'output_tokens': 1650, 'total_tokens': 1656, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 897}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is LLM?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e891b2b",
   "metadata": {},
   "source": [
    "### RAG Modern Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ffd41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate \n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24472df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001BE5CAB6830>, search_kwargs={})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_retriever = vector_db.as_retriever(\n",
    "    search_kargs={\"k\":3}\n",
    ")\n",
    "vector_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bb35aa",
   "metadata": {},
   "source": [
    "### A chatprompt template \n",
    "- Basically why I am usin this is because we need to talk to LLM at the end, so we need a prompt for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b0bbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\" You are a assistant for question-answering tasks. \n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum to answer and keep it concise.\n",
    "Context : {context}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "673f3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e42d44",
   "metadata": {},
   "source": [
    "- Stuffing releveant context from db in create stuff document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c66cb7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\" You are a assistant for question-answering tasks. \\nUse the following pieces of context to answer the question at the end.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nUse three sentences maximum to answer and keep it concise.\\nContext : {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001BE10A27A00>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83aa99f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001BE5CAB6830>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\" You are a assistant for question-answering tasks. \\nUse the following pieces of context to answer the question at the end.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nUse three sentences maximum to answer and keep it concise.\\nContext : {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001BE10A27A00>, default_metadata=(), model_kwargs={})\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Final RAG Chain\n",
    "rag_chain = create_retrieval_chain(vector_retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361ad39",
   "metadata": {},
   "source": [
    "- Invoking Final rag chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f90f5288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Where is MLR Institute of Technology located?',\n",
       " 'context': [Document(metadata={}, page_content='LLMs with external data.'),\n",
       "  Document(metadata={}, page_content='requirements.'),\n",
       "  Document(metadata={}, page_content='It provides abstractions for chains, agents, and memory, making it easier to integrate LLMs with'),\n",
       "  Document(metadata={}, page_content='It is widely applied in image recognition, natural language processing, and speech recognition.')],\n",
       " 'answer': \"I'm sorry, but the provided context does not contain information about the location of the MLR Institute of Technology. Therefore, I don't know the answer to your question.\"}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = rag_chain.invoke({\"input\":\"Where is MLR Institute of Technology located?\"})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ca9b1",
   "metadata": {},
   "source": [
    "### RAG Chain - Using LangChain Expression Language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eebf9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableParallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "855a6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\" \n",
    "    You are a assistant for question-answering tasks. \n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum to answer and keep it concise.\n",
    "    Context : {context}\n",
    "    Question : {input} \n",
    "    Answer : \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df280df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuff_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f027e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_lcel = (\n",
    "    {\"context\" : vector_retriever | stuff_docs,\n",
    "    \"question\" : RunnablePassthrough()\n",
    "    }\n",
    "    | custom_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49b8aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG, or Retrieval-Augmented Generation, combines a retrieval system with a generative model. This approach typically involves embedding documents and storing them in a vector database. Embeddings are numerical vector representations of text that allow semantic comparisons.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = rag_chain.invoke({\"input\" : \"What is RAG?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af746b2a",
   "metadata": {},
   "source": [
    "### Adding new data to VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381e0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = \"\"\" \n",
    "    NodeJs is a JavaScript runtime built on Chrome's V8 JavaScript engine. It allows developers to run JavaScript code on the server side, enabling the creation of scalable network applications. NodeJs is known for its event-driven, non-blocking I/O model, which makes it efficient and suitable for real-time applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13261073",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = Document(\n",
    "    page_content=new_document,\n",
    "    metadata={\"source\":\"copilot\",\"topic\": \"NodeJs\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f69e6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chunks = text_splitter.split_documents([new_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "597e9cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e724673b-68ba-41a3-85c1-b1eee0e33d94',\n",
       " '8f4e1689-9ac1-4e92-a522-82c5fd2628c4',\n",
       " 'aef3678c-0dd4-402a-b418-55c9195818a6',\n",
       " 'db58bfad-5f7d-4d15-80bf-f3ae0cbb888f']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.add_documents(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd0d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
