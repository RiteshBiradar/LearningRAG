{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8cb0e9",
   "metadata": {},
   "source": [
    "### RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f47859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = [\n",
    "  \"\"\"Deep learning is a subfield of machine learning that uses neural networks with multiple layers. It is widely applied in image recognition, natural language processing, and speech recognition.\"\"\",\n",
    "\n",
    "  \"\"\"Machine learning is the process of teaching computers to make decisions without being explicitly programmed. It includes supervised, unsupervised, and reinforcement learning approaches.\"\"\",\n",
    "\n",
    "  \"\"\"Retrieval-Augmented Generation (RAG) combines a retrieval system with a generative model. It improves accuracy by retrieving relevant documents from a knowledge base before generating answers.\"\"\",\n",
    "\n",
    "  \"\"\"LangChain is a framework designed to build applications powered by large language models. It provides abstractions for chains, agents, and memory, making it easier to integrate LLMs with external data.\"\"\",\n",
    "\n",
    "  \"\"\"When using RAG with LangChain, the pipeline typically involves embedding documents, storing them in a vector database, retrieving relevant chunks, and passing them into the LLM for context-aware generation.\"\"\",\n",
    "\n",
    "  \"\"\"Popular vector databases used with LangChain for RAG pipelines include Pinecone, Weaviate, FAISS, and Milvus.\"\"\",\n",
    "\n",
    "  \"\"\"Deep learning models often require GPUs for efficient training because of their high computational requirements.\"\"\",\n",
    "\n",
    "  \"\"\"LangChain supports integration with multiple LLM providers, including OpenAI, Anthropic, and Hugging Face.\"\"\",\n",
    "\n",
    "  \"\"\"In a RAG pipeline, embeddings are numerical vector representations of text that allow semantic similarity search.\"\"\",\n",
    "\n",
    "  \"\"\"Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets to improve accuracy.\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff9ed1",
   "metadata": {},
   "source": [
    "### Document Loading\n",
    "- But as I am using a dummy data over here, we can skip this part , or store it in a folder data and use directoryloader to import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "976dd5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Deep learning is a subfield of machine learning that uses neural networks with multiple layers. It is widely applied in image recognition, natural language processing, and speech recognition.'),\n",
       " Document(metadata={}, page_content='Machine learning is the process of teaching computers to make decisions without being explicitly programmed. It includes supervised, unsupervised, and reinforcement learning approaches.'),\n",
       " Document(metadata={}, page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a generative model. It improves accuracy by retrieving relevant documents from a knowledge base before generating answers.'),\n",
       " Document(metadata={}, page_content='LangChain is a framework designed to build applications powered by large language models. It provides abstractions for chains, agents, and memory, making it easier to integrate LLMs with external data.'),\n",
       " Document(metadata={}, page_content='When using RAG with LangChain, the pipeline typically involves embedding documents, storing them in a vector database, retrieving relevant chunks, and passing them into the LLM for context-aware generation.'),\n",
       " Document(metadata={}, page_content='Popular vector databases used with LangChain for RAG pipelines include Pinecone, Weaviate, FAISS, and Milvus.'),\n",
       " Document(metadata={}, page_content='Deep learning models often require GPUs for efficient training because of their high computational requirements.'),\n",
       " Document(metadata={}, page_content='LangChain supports integration with multiple LLM providers, including OpenAI, Anthropic, and Hugging Face.'),\n",
       " Document(metadata={}, page_content='In a RAG pipeline, embeddings are numerical vector representations of text that allow semantic similarity search.'),\n",
       " Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets to improve accuracy.')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "documents = [Document(page_content=doc) for doc in sample_docs] \n",
    "documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16934af",
   "metadata": {},
   "source": [
    "### Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1e8f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 23\n",
      "page_content='Retrieval-Augmented Generation (RAG) combines a retrieval system with a generative model. It'\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(chunks[4])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccceddd",
   "metadata": {},
   "source": [
    "### Embeddings using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c116ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03447727486491203, 0.03102317824959755, 0.006734970025718212, 0.026108985766768456, -0.03936202451586723, -0.16030244529247284, 0.06692401319742203, -0.006441489793360233, -0.0474504791200161, 0.014758856035768986, 0.07087527960538864, 0.05552763119339943, 0.019193334504961967, -0.026251312345266342, -0.01010954286903143, -0.02694045566022396, 0.022307461127638817, -0.022226648405194283, -0.14969263970851898, -0.017493007704615593, 0.00767625542357564, 0.05435224249958992, 0.0032543970737606287, 0.031725890934467316, -0.0846213847398758, -0.02940601296722889, 0.05159561336040497, 0.04812406003475189, -0.0033148222137242556, -0.058279167860746384, 0.04196927323937416, 0.022210685536265373, 0.1281888335943222, -0.022338971495628357, -0.011656315997242928, 0.06292839348316193, -0.032876335084438324, -0.09122604131698608, -0.031175347045063972, 0.0526994913816452, 0.04703482985496521, -0.08420311659574509, -0.030056199058890343, -0.02074483036994934, 0.009517835453152657, -0.0037217906210571527, 0.007343285251408815, 0.03932438790798187, 0.0932740643620491, -0.003788596484810114, -0.052742067724466324, -0.05805816128849983, -0.006864361464977264, 0.005283191800117493, 0.0828929990530014, 0.019362755119800568, 0.0062844837084412575, -0.010330787859857082, 0.009032378904521465, -0.037683695554733276, -0.04520607739686966, 0.024016305804252625, -0.006944137159734964, 0.013491630554199219, 0.10005494207143784, -0.07168391346931458, -0.021695120260119438, 0.031618405133485794, -0.051634665578603745, -0.08224772661924362, -0.06569333374500275, -0.00989533495157957, 0.005816374905407429, 0.07355456054210663, -0.034050311893224716, 0.0248861201107502, 0.014488042332231998, 0.02645738422870636, 0.009656722657382488, 0.0302172489464283, 0.05280393362045288, -0.07535984367132187, 0.009897145442664623, 0.029836809262633324, 0.01755557768046856, 0.023091984912753105, 0.001933806692250073, 0.0014002545503899455, -0.04717595875263214, -0.011194315738976002, -0.11420144140720367, -0.019811924546957016, 0.040266189724206924, 0.0021929906215518713, -0.07979220896959305, -0.02538231760263443, 0.09448299556970596, -0.02898104302585125, -0.14500252902507782, 0.23097744584083557, 0.027731187641620636, 0.03211146965622902, 0.03106505796313286, 0.04283284768462181, 0.06423777341842651, 0.03216316178441048, -0.004876770544797182, 0.055699463933706284, -0.03753238171339035, -0.02150554023683071, -0.028342634439468384, -0.028846951201558113, 0.0383530892431736, -0.017468664795160294, 0.052485305815935135, -0.07487601786851883, -0.03125976398587227, 0.021841565147042274, -0.03989570587873459, -0.008587091229856014, 0.026956576853990555, -0.04849553853273392, 0.011469882912933826, 0.02961820363998413, -0.02057218924164772, 0.013103843666613102, 0.028833510354161263, -3.1941990848222185e-33, 0.06478213518857956, -0.018130183219909668, 0.051789961755275726, 0.12198275327682495, 0.028780106455087662, 0.008721951395273209, -0.07052119821310043, -0.016907278448343277, 0.04073973000049591, 0.042116157710552216, 0.025447236374020576, 0.03574628755450249, -0.04914471507072449, 0.0021290204022079706, -0.015546582639217377, 0.050730545073747635, -0.0481853261590004, 0.03588061034679413, -0.0040670474991202354, 0.10172472149133682, -0.05597002059221268, -0.010681048966944218, 0.01123578567057848, 0.09068653732538223, 0.004234451334923506, 0.035138655453920364, -0.009702847339212894, -0.09386517852544785, 0.0928555428981781, 0.008004927076399326, -0.007705425377935171, -0.05208674445748329, -0.012587991543114185, 0.0032669377978891134, 0.006013509817421436, 0.007581559009850025, 0.01051718182861805, -0.08634556829929352, -0.06987880170345306, -0.0025338928680866957, -0.09097658842802048, 0.04688733071088791, 0.052076540887355804, 0.007193844299763441, 0.010903622955083847, -0.0052295587956905365, 0.013937311246991158, 0.021968349814414978, 0.03420866280794144, 0.060224682092666626, 0.00011665470083244145, 0.014731976203620434, -0.07008926570415497, 0.028499048203229904, -0.02760172076523304, 0.010768445208668709, 0.034830961376428604, -0.02248787134885788, 0.009769017808139324, 0.07722785323858261, 0.021588314324617386, 0.11495620757341385, -0.0680011734366417, 0.02376098558306694, -0.0159839428961277, -0.0178269911557436, 0.06439495831727982, 0.032025739550590515, 0.05027025192975998, -0.005913770757615566, -0.03370805084705353, 0.017840256914496422, 0.016573317348957062, 0.06329657882452011, 0.03467721864581108, 0.046473488211631775, 0.09790610522031784, -0.00663550291210413, 0.02520712837576866, -0.07798824459314346, 0.0169264767318964, -0.000945797364693135, 0.022471921518445015, -0.038253191858530045, 0.09570474177598953, -0.005350803025066853, 0.010469110682606697, -0.11524055153131485, -0.013262521475553513, -0.010709455236792564, -0.08311725407838821, 0.07327353954315186, 0.04939225688576698, -0.008994322270154953, -0.09584552049636841, 3.3661485617505796e-33, 0.12493184208869934, 0.01934972032904625, -0.05822571739554405, -0.03598826378583908, -0.05074676498770714, -0.04566238448023796, -0.08260336518287659, 0.1481948047876358, -0.08842118829488754, 0.06027443706989288, 0.05103015899658203, 0.01030314713716507, 0.14121422171592712, 0.03081384487450123, 0.061033159494400024, -0.052851270884275436, 0.1366489678621292, 0.00918989721685648, -0.01732518896460533, -0.012848555110394955, -0.007995282299816608, -0.0509800985455513, -0.05235064774751663, 0.007593012880533934, -0.015166307799518108, 0.01696030981838703, 0.021270520985126495, 0.020558107644319534, -0.12002813816070557, 0.014461833983659744, 0.02675991877913475, 0.025330696254968643, -0.0427546352148056, 0.006768387276679277, -0.01445858459919691, 0.04526195675134659, -0.09147648513317108, -0.019439145922660828, -0.017833467572927475, -0.05491018295288086, -0.052641112357378006, -0.010459048673510551, -0.052016086876392365, 0.020891955122351646, -0.07997036725282669, -0.012111340649425983, -0.05773142725229263, 0.023178234696388245, -0.008031732402741909, -0.02598930336534977, -0.07995671033859253, -0.020728832110762596, 0.048817697912454605, -0.020389137789607048, -0.04917657747864723, 0.014159622602164745, -0.06362202018499374, -0.007807393092662096, 0.01643155701458454, -0.0256824791431427, 0.013381040655076504, 0.026248741894960403, 0.009978413581848145, 0.06322886794805527, 0.002672201255336404, -0.006582767236977816, 0.01663188263773918, 0.03236646577715874, 0.03794245794415474, -0.036376070231199265, -0.006910930387675762, 0.0001596928050275892, -0.0016335808904841542, -0.02727821283042431, -0.02803807333111763, 0.049681417644023895, -0.028867173939943314, -0.0024180689360946417, 0.014774908311665058, 0.009764534421265125, 0.005797638092190027, 0.013486160896718502, 0.0055678957141935825, 0.03722710534930229, 0.007232527248561382, 0.04015626385807991, 0.08150326460599899, 0.07199167460203171, -0.013056126423180103, -0.0428820475935936, -0.01101123820990324, 0.004897820297628641, -0.009229730814695358, 0.035191506147384644, -0.05103502422571182, -1.571437557856825e-08, -0.08862441033124924, 0.02390925958752632, -0.01623876392841339, 0.031700510531663895, 0.027284247800707817, 0.05246882885694504, -0.047070957720279694, -0.058847445994615555, -0.06320822983980179, 0.04088849574327469, 0.04982800409197807, 0.10655171424150467, -0.07450230419635773, -0.012495421804487705, 0.01837071217596531, 0.03947412595152855, -0.024797886610031128, 0.014516262337565422, -0.03706921637058258, 0.02001572772860527, -4.85817035951186e-05, 0.00986657664179802, 0.024838753044605255, -0.05245814099907875, 0.029314178973436356, -0.08719190955162048, -0.01449968758970499, 0.026019077748060226, -0.01874636672437191, -0.07620512694120407, 0.03504333272576332, 0.10363949835300446, -0.028050510212779045, 0.012718182988464832, -0.07632549107074738, -0.01865232177078724, 0.02497672848403454, 0.0814453512430191, 0.06875883787870407, -0.0640566498041153, -0.08389385789632797, 0.06136231869459152, -0.033545564860105515, -0.10615336894989014, -0.040080588310956955, 0.032530225813388824, 0.07662483304738998, -0.0730162113904953, 0.00033755265758372843, -0.040871646255254745, -0.0757884755730629, 0.027527665719389915, 0.07462543249130249, 0.01771726831793785, 0.09121846407651901, 0.11022016406059265, 0.0005697731394320726, 0.05146336182951927, -0.014551310800015926, 0.03323203697800636, 0.023792240768671036, -0.02288980595767498, 0.038937538862228394, 0.030206844210624695]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") \n",
    "vector = embeddings.embed_query(\"Hello world\")\n",
    "print(vector)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880b740",
   "metadata": {},
   "source": [
    "### Initalize ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78443eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the collection: 157\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "persist_dir = \"./chroma_db\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"rag_collection\",\n",
    "    persist_directory=persist_dir,\n",
    ")\n",
    "vector_db.persist()\n",
    "\n",
    "print(f\"Number of documents in the collection: {vector_db._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107252d",
   "metadata": {},
   "source": [
    "### Testing Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea35b2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets'),\n",
       " Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets'),\n",
       " Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Fine tuning?\"\n",
    "result = vector_db.similarity_search(query,k=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8a83e",
   "metadata": {},
   "source": [
    "### Advance Similarity Search with Scores\n",
    "- Lower the value closer to the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "466210b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets'),\n",
       "  1.1072428226470947),\n",
       " (Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets'),\n",
       "  1.1072428226470947),\n",
       " (Document(metadata={}, page_content='Fine-tuning machine learning models involves adjusting model parameters on domain-specific datasets'),\n",
       "  1.1072428226470947)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = vector_db.similarity_search_with_score(query,k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf2ae7",
   "metadata": {},
   "source": [
    "### Initalizing LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31b82323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001C335659B40>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bc37a6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**LLM stands for Large Language Model.**\\n\\nIn simple terms, an LLM is a type of **artificial intelligence (AI) program** that has been trained on an enormous amount of text data to understand, generate, and process human-like language.\\n\\nLet\\'s break down the name:\\n\\n1.  **Large:** This refers to two main things:\\n    *   **Data:** LLMs are trained on truly massive datasets of text, often comprising trillions of words from the internet (web pages, books, articles, code, conversations, etc.).\\n    *   **Parameters:** They have billions, sometimes even trillions, of internal variables (parameters) that they adjust during training. The more parameters, the more complex patterns and nuances they can learn, leading to more sophisticated and human-like output.\\n\\n2.  **Language:** This indicates their primary focus: natural human language. They are designed to work with words, sentences, paragraphs, and the intricate rules and meanings that govern human communication.\\n\\n3.  **Model:** This signifies that it\\'s a mathematical construct or a computer program that learns patterns from data. It\\'s not a sentient being, but a sophisticated prediction engine.\\n\\n---\\n\\n**How do LLMs work (in a simplified way)?**\\n\\nAt their core, LLMs are trained to **predict the next word** in a sequence. Imagine trying to complete a sentence. By repeatedly trying to predict the next word across vast amounts of text, the model learns:\\n\\n*   **Grammar and Syntax:** How words fit together correctly.\\n*   **Facts and World Knowledge:** Information embedded in the training data.\\n*   **Context:** How the meaning of words changes based on surrounding words.\\n*   **Reasoning Patterns:** Implicit connections and logical structures within language.\\n*   **Writing Styles:** Different ways people express themselves.\\n\\nWhen you give an LLM a \"prompt\" (a question or instruction), it uses all this learned knowledge to generate a response, word by word, trying to produce the most probable and coherent sequence of text based on its training.\\n\\n---\\n\\n**Key Characteristics and Capabilities of LLMs:**\\n\\n*   **Generative:** They can create new, original content (text, code, stories, summaries, etc.).\\n*   **Contextual Understanding:** They can maintain context over long conversations or documents.\\n*   **Versatility:** A single LLM can perform a wide range of tasks without needing specific retraining for each one (e.g., answer questions, summarize, translate, write code, brainstorm ideas).\\n*   **Few-shot/Zero-shot Learning:** They can often perform new tasks effectively even with very few or no specific examples, just by being prompted well.\\n*   **Emergent Abilities:** As models scale up in size and data, they sometimes develop capabilities that weren\\'t explicitly programmed or expected.\\n\\n---\\n\\n**Examples of well-known LLMs:**\\n\\n*   **OpenAI\\'s GPT series** (e.g., GPT-3, GPT-3.5, GPT-4, which power ChatGPT)\\n*   **Google\\'s PaLM and Gemini**\\n*   **Meta\\'s LLaMA**\\n*   **Anthropic\\'s Claude**\\n\\n---\\n\\n**In essence, LLMs are powerful AI systems capable of understanding and generating human language, making them incredibly versatile tools for a vast array of applications, from writing assistance to information retrieval and creative content generation.**', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--45cf116a-d927-446e-8698-d1ec50ea5618-0', usage_metadata={'input_tokens': 6, 'output_tokens': 1974, 'total_tokens': 1980, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1256}})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is LLM?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e891b2b",
   "metadata": {},
   "source": [
    "### RAG Modern Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ffd41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate \n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "24472df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001C33565BD30>, search_kwargs={})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_retriever = vector_db.as_retriever(\n",
    "    search_kargs={\"k\":3}\n",
    ")\n",
    "vector_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bb35aa",
   "metadata": {},
   "source": [
    "### A chatprompt template \n",
    "- Basically why I am usin this is because we need to talk to LLM at the end, so we need a prompt for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b0bbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\" You are a assistant for question-answering tasks. \n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum to answer and keep it concise.\n",
    "Context : {context}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "673f3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e42d44",
   "metadata": {},
   "source": [
    "- Stuffing releveant context from db in create stuff document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c66cb7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\" You are a assistant for question-answering tasks. \\nUse the following pieces of context to answer the question at the end.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nUse three sentences maximum to answer and keep it concise.\\nContext : {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001C335659B40>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "83aa99f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001C33565BD30>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\" You are a assistant for question-answering tasks. \\nUse the following pieces of context to answer the question at the end.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nUse three sentences maximum to answer and keep it concise.\\nContext : {context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001C335659B40>, default_metadata=(), model_kwargs={})\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Final RAG Chain\n",
    "rag_chain = create_retrieval_chain(vector_retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361ad39",
   "metadata": {},
   "source": [
    "- Invoking Final rag chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f90f5288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Where is MLR Institute of Technology located?',\n",
       " 'context': [Document(metadata={'topic': 'Machine Learning', 'source': 'copilot'}, page_content='Machine learning is a field of artificial intelligence that focuses on enabling computers to learn'),\n",
       "  Document(metadata={'source': 'copilot', 'topic': 'Machine Learning'}, page_content='Machine learning is a field of artificial intelligence that focuses on enabling computers to learn'),\n",
       "  Document(metadata={}, page_content='LLMs with external data.'),\n",
       "  Document(metadata={}, page_content='LLMs with external data.')],\n",
       " 'answer': \"I don't know the answer to where MLR Institute of Technology is located. The provided context does not contain information about educational institutions or their locations.\"}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = rag_chain.invoke({\"input\":\"Where is MLR Institute of Technology located?\"})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ca9b1",
   "metadata": {},
   "source": [
    "### RAG Chain - Using LangChain Expression Language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eebf9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableParallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "855a6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\" \n",
    "    You are a assistant for question-answering tasks. \n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum to answer and keep it concise.\n",
    "    Context : {context}\n",
    "    Question : {input} \n",
    "    Answer : \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "df280df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stuff_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4f027e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_lcel = (\n",
    "    {\"context\" : vector_retriever | stuff_docs,\n",
    "    \"question\" : RunnablePassthrough()\n",
    "    }\n",
    "    | custom_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49b8aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize, but the provided context does not define what RAG stands for or what it is. It only describes a pipeline involving RAG with LangChain.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = rag_chain.invoke({\"input\" : \"What is RAG?\"})\n",
    "results['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af746b2a",
   "metadata": {},
   "source": [
    "### Adding new data to VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "381e0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = \"\"\" \n",
    "    NodeJs is a JavaScript runtime built on Chrome's V8 JavaScript engine. It allows developers to run JavaScript code on the server side, enabling the creation of scalable network applications. NodeJs is known for its event-driven, non-blocking I/O model, which makes it efficient and suitable for real-time applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13261073",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = Document(\n",
    "    page_content=new_document,\n",
    "    metadata={\"source\":\"copilot\",\"topic\": \"NodeJs\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f69e6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chunks = text_splitter.split_documents([new_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "597e9cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cacb1163-36f7-4d97-afdb-d5a2f32cd0c6',\n",
       " '7dde22e4-ef31-49e5-bc52-a52ed3322148',\n",
       " '462e66c8-47b8-4d52-8ab2-859b6106d70f',\n",
       " '312a486f-c3aa-4206-a117-1c1507509641']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.add_documents(new_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32aceb",
   "metadata": {},
   "source": [
    "### Advance RAG Technique - Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eecd0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7a15e",
   "metadata": {},
   "source": [
    "- We need this to remember previous context\n",
    "- create_history_aware it makes the retriever understand conversation context\n",
    "- MessagePlaceHolder - place holder for previous data/ chat history\n",
    "- Human - input, AI - output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "be0c6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = \"\"\" Machine learning is a field of artificial intelligence that focuses on enabling computers to learn from data and improve their performance on specific tasks without being explicitly programmed. It involves the development of algorithms and statistical models that can analyze and interpret complex datasets to make predictions or decisions. Machine learning is widely used in various applications, including image recognition, natural language processing, recommendation systems, and autonomous vehicles. Types of Machine Learning: Supervised Learning, Unsupervised Learning, Reinforcement Learning. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c1233133",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = Document(\n",
    "    page_content=new_document,\n",
    "    metadata={\"source\":\"copilot\",\"topic\": \"Machine Learning\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "36e9b824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b14d329c-f8db-4dc3-afdf-76ce044db58f',\n",
       " '9099f95b-1d1f-482a-b8f0-a6b649409f46',\n",
       " '7fc0b351-4ee1-4778-be72-4049a7484b1c',\n",
       " '48e690b1-498b-4e29-8036-a70380ac3a5b',\n",
       " '771d3364-7d7b-47da-97dd-9eb3a69496f9',\n",
       " '11b90ba9-7ff9-472d-ba69-586c5e68b8cd',\n",
       " 'd0536bd3-1b39-4e8b-896f-8c04661f8785']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chunks = text_splitter.split_documents([new_doc])\n",
    "vector_db.add_documents(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4161bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = \"\"\" Types of fruits are apples, bananas, oranges, grapes, and strawberries. Fruits are a rich source of vitamins, minerals, and fiber, making them an essential part of a healthy diet. They can be consumed fresh, dried, or juiced, and are often used in cooking and baking to add natural sweetness and flavor. Regular consumption of fruits is associated with numerous health benefits, including improved digestion, reduced risk of chronic diseases, and enhanced immune function. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a7233341",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = Document(\n",
    "    page_content=new_document,\n",
    "    metadata={\"source\":\"copilot\",\"topic\": \"Fruits\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e6722e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6d13c96d-20c1-4f2c-9cd7-e9dfce3e3f7b',\n",
       " '2aed39c3-5f10-48ef-9c27-c92b108d75a7',\n",
       " 'edea1d9c-1cda-42be-8472-0b6df1e2f4ae',\n",
       " 'c208d6fa-3953-413a-94a5-708c0be08acb',\n",
       " '1c45d82b-5cd6-4394-bbaf-d29ecab6d0e7',\n",
       " '8c34eae9-0a06-4e37-8f83-0a83126654c1']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chunks = text_splitter.split_documents([new_doc])\n",
    "vector_db.add_documents(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d4ecdee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\" Given a chat history and  the latest user question which might reference context in chat history, formulate a standalone question that can be understood without the chat history.\n",
    " Do NOT answer the question, just reformulate it if needed and otherwise return as it is .\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5774fcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001C33565BD30>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001C37C441360>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=' Given a chat history and  the latest user question which might reference context in chat history, formulate a standalone question that can be understood without the chat history.\\n Do NOT answer the question, just reformulate it if needed and otherwise return as it is .'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001C335659B40>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001C33565BD30>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, vector_retriever, contextualize_q_prompt\n",
    ")\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e9e684ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\" You are a assistant for question-answering tasks. \n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum to answer and keep it concise.\n",
    "Context : {context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm,qa_prompt)\n",
    "\n",
    "conversational_rag_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, question_answer_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "300a29ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a field of artificial intelligence that focuses on enabling computers to learn. It involves teaching computers to make decisions without being explicitly programmed.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "result1 = conversational_rag_chain.invoke(\n",
    "    {\"chat_history\": chat_history,\n",
    "     \"input\":\"What is Machine Learning?\"\n",
    "    }\n",
    ")\n",
    "result1['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3354b83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = chat_history[:-2]\n",
    "chat_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8af4b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([ \n",
    "    HumanMessage(content=\"What is Machine Learning?\"),\n",
    "    AIMessage(content=result1['answer'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "181ee391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is Machine Learning?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Machine learning is a field of artificial intelligence that focuses on enabling computers to learn. It involves teaching computers to make decisions without being explicitly programmed.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1642eceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know the answer based on the provided context. The context explains what Machine Learning is but does not mention its different types.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = conversational_rag_chain.invoke(\n",
    "    {\"input\":\"What are it different types?\", \"chat_history\": chat_history}\n",
    ")\n",
    "result2['answer'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d0e35c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_document = \"\"\" \n",
    "    List in html is used to group a set of related items. There are two types of lists in HTML: unordered lists (ul) and ordered lists (ol). Unordered lists\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bec4ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = Document(\n",
    "    page_content=new_document,\n",
    "    metadata={\"source\":\"copilot\",\"topic\": \"Lists\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8c8f06dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11531927-37c1-413b-a998-04e83ef04fd0',\n",
       " '43c0e45f-08fb-405f-a146-65ce2ea6c565']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chunks = text_splitter.split_documents([new_doc])\n",
    "vector_db.add_documents(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa26165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A list in HTML is used to group a set of related items. There are two types of lists in HTML: unordered lists (ul) and ordered lists (ol).'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = conversational_rag_chain.invoke(\n",
    "    {\"chat_history\": chat_history,\n",
    "     \"input\":\"What is List in html?\"\n",
    "    }\n",
    ")\n",
    "result1['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([\n",
    "    HumanMessage(content=\"What is List in html?\"), AIMessage(content=result1['answer'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "177e244e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is Machine Learning?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Machine learning is a field of artificial intelligence that focuses on enabling computers to learn. It involves teaching computers to make decisions without being explicitly programmed.', additional_kwargs={}, response_metadata={}),\n",
       " (HumanMessage(content='What is List in html?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='A list in HTML is used to group a set of related items. There are two types of lists in HTML: unordered lists (ul) and ordered lists (ol).', additional_kwargs={}, response_metadata={}))]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f80abc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is Machine Learning?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Machine learning is a field of artificial intelligence that focuses on enabling computers to learn. It involves teaching computers to make decisions without being explicitly programmed.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is List in html?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='A list in HTML is used to group a set of related items. There are two types of lists in HTML: unordered lists (ul) and ordered lists (ol).', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = []\n",
    "for item in chat_history:\n",
    "    if isinstance(item, tuple):\n",
    "        flattened.extend(item)\n",
    "    else:\n",
    "        flattened.append(item)\n",
    "chat_history = flattened\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3e7b1201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The different types of lists in HTML are unordered lists (ul) and ordered lists (ol).'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = conversational_rag_chain.invoke({\n",
    "    \"input\":\"What are it different types?\", \n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "result2['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
